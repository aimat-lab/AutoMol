{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "import os\n",
    "\n",
    "\n",
    "def load_env():\n",
    "    home = expanduser(\"~\")\n",
    "    print(home)\n",
    "    with open(home+'/.env', 'r') as f:\n",
    "        env = dict()\n",
    "        for line in f.readlines():\n",
    "            key, value = line.split('=')\n",
    "            env[key] = value.split('\\n')[0]\n",
    "        return env\n",
    "\n",
    "def export_env():\n",
    "    \"\"\"Loads your .env file and exports the three variables important for mlflow.\n",
    "    MLFLOW_TRACKING_USERNAME, MLFLOW_TRACKING_PASSWORD, MLFLOW_TRACKING_URI\n",
    "    \"\"\"\n",
    "    env = load_env()\n",
    "    for key in [\"MLFLOW_TRACKING_USERNAME\", \"MLFLOW_TRACKING_PASSWORD\", \"MLFLOW_TRACKING_URI\"]:\n",
    "        os.environ[key] = env[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlflow_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d33e24afac54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mexport_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlflow_utils'"
     ]
    }
   ],
   "source": [
    "### RUN FIRST:\n",
    "### sudo openvpn --config kit.ovpn\n",
    "\n",
    "import mlflow\n",
    "from mlflow_utils.load_env import export_env\n",
    "\n",
    "export_env()\n",
    "mlflow.set_experiment(\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coloumb matimport pysftp\n",
    "import paramiko\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class GDB9_data_instance:\n",
    "    def __init__(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            raise Exception(\"Path {} does not exist.\".format(path))\n",
    "        with open(path, 'r') as f:\n",
    "            num_of_atoms = int(f.readline())\n",
    "            self.scalar_properties = f.readline().strip().replace(\"gdb \",\"\").split(\"\\t\")\n",
    "            self.scalar_properties = [float(p) for p in self.scalar_properties]\n",
    "            self.scalar_properties[0] = int(self.scalar_properties[0])\n",
    "            self.atoms = []\n",
    "            for i in range(num_of_atoms):\n",
    "                atom_posXYZ_MullikenPartialCharge = f.readline().strip().replace(\" \",\"\").replace(\"*^\",\"e\").split(\"\\t\")\n",
    "                self.atoms.append(atom_posXYZ_MullikenPartialCharge)\n",
    "            self.harmonic_vibrational_frequencies = f.readline().strip().split(\"\\t\")\n",
    "            (self.gdb17_smile, self.b3lyp_relaxation_smile) = f.readline().strip().split(\"\\t\")\n",
    "            (self.corina_InChI, self.b3lyp_InChI) = f.readline().strip().split(\"\\t\")\n",
    "        \n",
    "        \n",
    "gdb9_data_dir = r\"dsgdb9nsd\"\n",
    "gdb9_data = []\n",
    "amount = 1000\n",
    "currently_checked = 0\n",
    "\n",
    "config = paramiko.config.SSHConfig()\n",
    "config.parse(open(os.path.expanduser('~/.ssh/config')))\n",
    "conf = config.lookup('lsdf')\n",
    "with pysftp.Connection(host=conf['hostname'], username=conf['user'], private_key=conf['identityfile'][0]) as sftp:\n",
    "    dataset_path = '/lsdf/kit/iti/projects/aimat-mlflow/datasets/dsgdb9nsd'\n",
    "    with sftp.cd(dataset_path):\n",
    "        #returned = sftp.execute('ls | wc -l')\n",
    "        #sftp.chdir(dataset_path)\n",
    "        cmd = 'cd ' + dataset_path + ' && ls | wc -l'\n",
    "        returned = sftp.execute(cmd)\n",
    "        num_files = [int(r.decode()) for r in returned][0]\n",
    "        print(\"Number of files in the dataset: {}\".format(num_files))\n",
    "        for file in sftp.listdir():\n",
    "            if len(gdb9_data) >= amount or currently_checked >= amount:\n",
    "                break\n",
    "            if file.endswith(\".xyz\"):\n",
    "                currently_checked += 1\n",
    "                #print(file)\n",
    "                file_path = os.path.join(dataset_path, file)\n",
    "                local_dir = os.getcwd() + '/data/' + gdb9_data_dir\n",
    "                if not os.path.exists(local_dir):\n",
    "                    os.makedirs(local_dir)\n",
    "                local_path = os.path.join(local_dir, file)\n",
    "                if not os.path.exists(local_path):\n",
    "                    sftp.get(file_path, local_path)\n",
    "                gdb9_data.append(GDB9_data_instance(local_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = [d.gdb17_smile for d in gdb9_data]\n",
    "#smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "homo_energies = np.array([d.scalar_properties[6] for d in gdb9_data])\n",
    "#homo_energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit.Chem as Chem\n",
    "\n",
    "fps = np.array([np.array(Chem.RDKFingerprint(Chem.MolFromSmiles(smi))).astype(float)\n",
    "                                    for smi in smiles])\n",
    "#fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = fps\n",
    "#test_features = \n",
    "\n",
    "train_labels = homo_energies\n",
    "#test_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(title, train_scores, test_scores, fit_times, \n",
    "                        axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
    "\n",
    "#model = MLPRegressor(random_state=1, max_iter=300)\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(model, train_features, train_labels, cv=cv, n_jobs=None, return_times=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "title = \"Learning Curves for \" + str(model)\n",
    "\n",
    "plot_learning_curve(title, train_scores, test_scores, fit_times, \n",
    "                    #axes=axes, ylim=(0.7, 1.01), cv=cv, n_jobs=4, train_sizes=train_sizes)\n",
    "                    axes=axes, cv=cv, n_jobs=4, train_sizes=train_sizes)\n",
    "\n",
    "## saving the current figure if neccesary ##\n",
    "# plt.savefig('foo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def fetch_logged_data(run_id):\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    data = client.get_run(run_id).data\n",
    "    tags = {k: v for k, v in data.tags.items() if not k.startswith(\"mlflow.\")}\n",
    "    artifacts = [f.path for f in client.list_artifacts(run_id, \"model\")]\n",
    "    return data.params, data.metrics, tags, artifacts\n",
    "\n",
    "params, metrics, tags, artifacts = fetch_logged_data(run.info.run_id)\n",
    "\n",
    "pprint(params)\n",
    "pprint(metrics)\n",
    "pprint(tags)\n",
    "pprint(artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automol",
   "language": "python",
   "name": "automol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
